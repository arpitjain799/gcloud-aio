<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>gcloud.aio.storage.storage API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>gcloud.aio.storage.storage</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import binascii
import enum
import io
import json
import logging
import mimetypes
import os
import sys
from typing import Any
from typing import AnyStr
from typing import Dict
from typing import IO
from typing import Iterator
from typing import List
from typing import Optional
from typing import Tuple
from typing import Union
from urllib.parse import quote

from gcloud.aio.auth import AioSession  # pylint: disable=no-name-in-module
from gcloud.aio.auth import BUILD_GCLOUD_REST  # pylint: disable=no-name-in-module
from gcloud.aio.auth import Token  # pylint: disable=no-name-in-module
from gcloud.aio.storage.bucket import Bucket
from gcloud.aio.storage.constants import DEFAULT_TIMEOUT

# Selectively load libraries based on the package
if BUILD_GCLOUD_REST:
    from time import sleep
    from requests import HTTPError as ResponseError
    from requests import Session
    from builtins import open as file_open
else:
    from aiofiles import open as file_open  # type: ignore[no-redef]
    from asyncio import sleep  # type: ignore[assignment]
    from aiohttp import (  # type: ignore[assignment]
        ClientResponseError as ResponseError)
    from aiohttp import ClientSession as Session  # type: ignore[assignment]

API_ROOT = &#39;https://www.googleapis.com/storage/v1/b&#39;
API_ROOT_UPLOAD = &#39;https://www.googleapis.com/upload/storage/v1/b&#39;
VERIFY_SSL = True
SCOPES = [
    &#39;https://www.googleapis.com/auth/devstorage.read_write&#39;,
]

MAX_CONTENT_LENGTH_SIMPLE_UPLOAD = 5 * 1024 * 1024  # 5 MB

STORAGE_EMULATOR_HOST = os.environ.get(&#39;STORAGE_EMULATOR_HOST&#39;)
if STORAGE_EMULATOR_HOST:
    API_ROOT = f&#39;https://{STORAGE_EMULATOR_HOST}/storage/v1/b&#39;
    API_ROOT_UPLOAD = f&#39;https://{STORAGE_EMULATOR_HOST}/upload/storage/v1/b&#39;
    VERIFY_SSL = False

log = logging.getLogger(__name__)


def choose_boundary() -&gt; str:
    &#34;&#34;&#34;Stolen from urllib3.filepost.choose_boundary() as of v1.26.2.&#34;&#34;&#34;
    boundary = binascii.hexlify(os.urandom(16))
    if sys.version_info.major == 2:
        return boundary  # type: ignore[return-value]
    return boundary.decode(&#39;ascii&#39;)


def encode_multipart_formdata(fields: List[Tuple[Dict[str, str], bytes]],
                              boundary: str) -&gt; Tuple[bytes, str]:
    &#34;&#34;&#34;
    Stolen from urllib3.filepost.encode_multipart_formdata() as of v1.26.2.

    Very heavily modified to be compatible with our gcloud-rest converter and
    to avoid unnecessary urllib3 dependencies (since that&#39;s only included with
    requests, not aiohttp).
    &#34;&#34;&#34;
    body: List[bytes] = []
    for headers, data in fields:
        body.append(f&#39;--{boundary}\r\n&#39;.encode(&#39;utf-8&#39;))

        # The below is from RequestFields.render_headers()
        # Since we only use Content-Type, we could simplify the below to a
        # single line... but probably best to be safe for future modifications.
        for field in [&#39;Content-Disposition&#39;, &#39;Content-Type&#39;,
                      &#39;Content-Location&#39;]:
            value = headers.pop(field, None)
            if value:
                body.append(f&#39;{field}: {value}\r\n&#39;.encode(&#39;utf-8&#39;))
        for field, value in headers.items():
            # N.B. potential bug copied from urllib3 code; zero values should
            # be sent! Keeping it for now, since Google libs use urllib3 for
            # their examples.
            if value:
                body.append(f&#39;{field}: {value}\r\n&#39;.encode(&#39;utf-8&#39;))

        body.append(b&#39;\r\n&#39;)
        body.append(data)
        body.append(b&#39;\r\n&#39;)

    body.append(f&#39;--{boundary}--\r\n&#39;.encode(&#39;utf-8&#39;))

    # N.B. &#39;multipart/form-data&#39; in upstream, but Google wants &#39;related&#39;
    content_type = f&#39;multipart/related; boundary={boundary}&#39;

    return b&#39;&#39;.join(body), content_type


class UploadType(enum.Enum):
    SIMPLE = 1
    RESUMABLE = 2
    MULTIPART = 3  # unused: SIMPLE upgrades to MULTIPART when metadata exists


class StreamResponse:
    &#34;&#34;&#34;This class provides an abstraction between the slightly different
    recommended streaming implementations between requests and aiohttp.
    &#34;&#34;&#34;

    def __init__(self, response: Any) -&gt; None:
        self._response = response
        self._iter: Optional[Iterator[bytes]] = None

    @property
    def content_length(self) -&gt; int:
        return int(self._response.headers.get(&#39;content-length&#39;, 0))

    async def read(self, size: int = -1) -&gt; bytes:
        chunk: bytes
        if BUILD_GCLOUD_REST:
            if self._iter is None:
                self._iter = self._response.iter_content(chunk_size=size)
            chunk = next(self._iter, b&#39;&#39;)
        else:
            chunk = await self._response.content.read(size)
        return chunk

    async def __aenter__(self) -&gt; Any:
        # strictly speaking, since this method can&#39;t be called via gcloud-rest,
        # we know the return type is aiohttp.ClientResponse
        return await self._response.__aenter__()

    async def __aexit__(self, *exc_info: Any) -&gt; None:
        await self._response.__aexit__(*exc_info)


class Storage:
    def __init__(self, *,
                 service_file: Optional[Union[str, IO[AnyStr]]] = None,
                 token: Optional[Token] = None,
                 session: Optional[Session] = None) -&gt; None:
        self.session = AioSession(session, verify_ssl=VERIFY_SSL)
        self.token = token or Token(
            service_file=service_file, scopes=SCOPES,
            session=self.session.session)  # type: ignore[arg-type]

    async def _headers(self) -&gt; Dict[str, str]:
        if STORAGE_EMULATOR_HOST:
            return {}

        token = await self.token.get()
        return {
            &#39;Authorization&#39;: f&#39;Bearer {token}&#39;,
        }

    def get_bucket(self, bucket_name: str) -&gt; Bucket:
        return Bucket(self, bucket_name)

    # pylint: disable=too-many-locals
    async def copy(self, bucket: str, object_name: str,
                   destination_bucket: str, *, new_name: Optional[str] = None,
                   metadata: Optional[Dict[str, Any]] = None,
                   params: Optional[Dict[str, str]] = None,
                   headers: Optional[Dict[str, str]] = None,
                   timeout: int = DEFAULT_TIMEOUT,
                   session: Optional[Session] = None) -&gt; Dict[str, Any]:

        &#34;&#34;&#34;
        When files are too large, multiple calls to `rewriteTo` are made. We
        refer to the same copy job by using the `rewriteToken` from the
        previous return payload in subsequent `rewriteTo` calls.

        Using the `rewriteTo` GCS API is preferred in part because it is able
        to make multiple calls to fully copy an object whereas the `copyTo` GCS
        API only calls `rewriteTo` once under the hood, and thus may fail if
        files are large.

        In the rare case you need to resume a copy operation, include the
        `rewriteToken` in the `params` dictionary. Once you begin a multi-part
        copy operation, you then have 1 week to complete the copy job.

        https://cloud.google.com/storage/docs/json_api/v1/objects/rewrite
        &#34;&#34;&#34;
        if not new_name:
            new_name = object_name

        url = (f&#34;{API_ROOT}/{bucket}/o/{quote(object_name, safe=&#39;&#39;)}/rewriteTo&#34;
               f&#34;/b/{destination_bucket}/o/{quote(new_name, safe=&#39;&#39;)}&#34;)

        # We may optionally supply metadata* to apply to the rewritten
        # object, which explains why `rewriteTo` is a POST endpoint; when no
        # metadata is given, we have to send an empty body.
        # * https://cloud.google.com/storage/docs/json_api/v1/objects#resource
        metadict = (metadata or {}).copy()
        metadict = {self._format_metadata_key(k): v
                    for k, v in metadict.items()}
        if &#39;metadata&#39; in metadict:
            metadict[&#39;metadata&#39;] = {
                str(k): str(v) if v is not None else None
                for k, v in metadict[&#39;metadata&#39;].items()}

        metadata_ = json.dumps(metadict)

        headers = headers or {}
        headers.update(await self._headers())
        headers.update({
            &#39;Content-Length&#39;: str(len(metadata_)),
            &#39;Content-Type&#39;: &#39;application/json; charset=UTF-8&#39;,
        })

        params = params or {}

        s = AioSession(session) if session else self.session
        resp = await s.post(url, headers=headers, params=params,
                            timeout=timeout, data=metadata_)

        data: Dict[str, Any] = await resp.json(content_type=None)

        while not data.get(&#39;done&#39;) and data.get(&#39;rewriteToken&#39;):
            params[&#39;rewriteToken&#39;] = data[&#39;rewriteToken&#39;]
            resp = await s.post(url, headers=headers, params=params,
                                timeout=timeout)
            data = await resp.json(content_type=None)

        return data

    async def delete(self, bucket: str, object_name: str, *,
                     timeout: int = DEFAULT_TIMEOUT,
                     params: Optional[Dict[str, str]] = None,
                     headers: Optional[Dict[str, str]] = None,
                     session: Optional[Session] = None) -&gt; str:
        # https://cloud.google.com/storage/docs/request-endpoints#encoding
        encoded_object_name = quote(object_name, safe=&#39;&#39;)
        url = f&#39;{API_ROOT}/{bucket}/o/{encoded_object_name}&#39;
        headers = headers or {}
        headers.update(await self._headers())

        s = AioSession(session) if session else self.session
        resp = await s.delete(url, headers=headers, params=params or {},
                              timeout=timeout)

        try:
            data: str = await resp.text()
        except (AttributeError, TypeError):
            data = str(resp.text)

        return data

    async def download(self, bucket: str, object_name: str, *,
                       headers: Optional[Dict[str, Any]] = None,
                       timeout: int = DEFAULT_TIMEOUT,
                       session: Optional[Session] = None) -&gt; bytes:
        return await self._download(bucket, object_name, headers=headers,
                                    timeout=timeout, params={&#39;alt&#39;: &#39;media&#39;},
                                    session=session)

    async def download_to_filename(self, bucket: str, object_name: str,
                                   filename: str, **kwargs: Any) -&gt; None:
        async with file_open(  # type: ignore[attr-defined]
                filename,
                mode=&#39;wb+&#39;,
        ) as file_object:
            await file_object.write(
                await self.download(bucket, object_name, **kwargs)
            )

    async def download_metadata(self, bucket: str, object_name: str, *,
                                headers: Optional[Dict[str, Any]] = None,
                                session: Optional[Session] = None,
                                timeout: int = DEFAULT_TIMEOUT
                                ) -&gt; Dict[str, Any]:
        data = await self._download(bucket, object_name, headers=headers,
                                    timeout=timeout, session=session)
        metadata: Dict[str, Any] = json.loads(data.decode())
        return metadata

    async def download_stream(self, bucket: str, object_name: str, *,
                              headers: Optional[Dict[str, Any]] = None,
                              timeout: int = DEFAULT_TIMEOUT,
                              session: Optional[Session] = None
                              ) -&gt; StreamResponse:
        &#34;&#34;&#34;Download a GCS object in a buffered stream.

        Args:
            bucket (str): The bucket from which to download.
            object_name (str): The object within the bucket to download.
            headers (Optional[Dict[str, Any]], optional): Custom header values
                for the request, such as range. Defaults to None.
            timeout (int, optional): Timeout, in seconds, for the request. Note
                that with this function, this is the time to the beginning of
                the response data (TTFB). Defaults to 10.
            session (Optional[Session], optional): A specific session to
                (re)use. Defaults to None.

        Returns:
            StreamResponse: A object encapsulating the stream, similar to
            io.BufferedIOBase, but it only supports the read() function.
        &#34;&#34;&#34;
        return await self._download_stream(bucket, object_name,
                                           headers=headers, timeout=timeout,
                                           params={&#39;alt&#39;: &#39;media&#39;},
                                           session=session)

    async def list_objects(self, bucket: str, *,
                           params: Optional[Dict[str, str]] = None,
                           headers: Optional[Dict[str, Any]] = None,
                           session: Optional[Session] = None,
                           timeout: int = DEFAULT_TIMEOUT) -&gt; Dict[str, Any]:
        url = f&#39;{API_ROOT}/{bucket}/o&#39;
        headers = headers or {}
        headers.update(await self._headers())

        s = AioSession(session) if session else self.session
        resp = await s.get(url, headers=headers, params=params or {},
                           timeout=timeout)
        data: Dict[str, Any] = await resp.json(content_type=None)
        return data

    # https://cloud.google.com/storage/docs/json_api/v1/how-tos/upload
    # pylint: disable=too-many-locals
    async def upload(self, bucket: str, object_name: str, file_data: Any,
                     *, content_type: Optional[str] = None,
                     parameters: Optional[Dict[str, str]] = None,
                     headers: Optional[Dict[str, str]] = None,
                     metadata: Optional[Dict[str, Any]] = None,
                     session: Optional[Session] = None,
                     force_resumable_upload: Optional[bool] = None,
                     timeout: int = 30) -&gt; Dict[str, Any]:
        url = f&#39;{API_ROOT_UPLOAD}/{bucket}/o&#39;

        stream = self._preprocess_data(file_data)

        if BUILD_GCLOUD_REST and isinstance(stream, io.StringIO):
            # HACK: `requests` library does not accept `str` as `data` in `put`
            # HTTP request.
            stream = io.BytesIO(stream.getvalue().encode(&#39;utf-8&#39;))

        content_length = self._get_stream_len(stream)

        # mime detection method same as in aiohttp 3.4.4
        content_type = content_type or mimetypes.guess_type(object_name)[0]

        parameters = parameters or {}

        headers = headers or {}
        headers.update(await self._headers())
        headers.update({
            &#39;Content-Length&#39;: str(content_length),
            &#39;Content-Type&#39;: content_type or &#39;&#39;,
        })

        upload_type = self._decide_upload_type(force_resumable_upload,
                                               content_length)
        log.debug(&#39;using %r gcloud storage upload method&#39;, upload_type)

        if upload_type == UploadType.RESUMABLE:
            return await self._upload_resumable(
                url, object_name, stream, parameters, headers,
                metadata=metadata, session=session, timeout=timeout)
        if upload_type == UploadType.SIMPLE:
            if metadata:
                return await self._upload_multipart(
                    url, object_name, stream, parameters, headers, metadata,
                    session=session, timeout=timeout)
            return await self._upload_simple(
                url, object_name, stream, parameters, headers, session=session,
                timeout=timeout)

        raise TypeError(f&#39;upload type {upload_type} not supported&#39;)

    async def upload_from_filename(self, bucket: str, object_name: str,
                                   filename: str,
                                   **kwargs: Any) -&gt; Dict[str, Any]:
        async with file_open(  # type: ignore[attr-defined]
                filename,
                mode=&#39;rb&#39;,
        ) as file_object:
            contents = await file_object.read()
            return await self.upload(bucket, object_name, contents,
                                     **kwargs)

    @staticmethod
    def _get_stream_len(stream: IO[AnyStr]) -&gt; int:
        current = stream.tell()
        try:
            return stream.seek(0, os.SEEK_END)
        finally:
            stream.seek(current)

    @staticmethod
    def _preprocess_data(data: Any) -&gt; IO[Any]:
        if data is None:
            return io.StringIO(&#39;&#39;)

        if isinstance(data, bytes):
            return io.BytesIO(data)
        if isinstance(data, str):
            return io.StringIO(data)
        if isinstance(data, io.IOBase):
            return data  # type: ignore[return-value]

        raise TypeError(f&#39;unsupported upload type: &#34;{type(data)}&#34;&#39;)

    @staticmethod
    def _decide_upload_type(force_resumable_upload: Optional[bool],
                            content_length: int) -&gt; UploadType:
        # force resumable
        if force_resumable_upload is True:
            return UploadType.RESUMABLE

        # force simple
        if force_resumable_upload is False:
            return UploadType.SIMPLE

        # decide based on Content-Length
        if content_length &gt; MAX_CONTENT_LENGTH_SIMPLE_UPLOAD:
            return UploadType.RESUMABLE

        return UploadType.SIMPLE

    @staticmethod
    def _split_content_type(content_type: str) -&gt; Tuple[str, Optional[str]]:
        content_type_and_encoding_split = content_type.split(&#39;;&#39;)
        content_type = content_type_and_encoding_split[0].lower().strip()

        encoding = None
        if len(content_type_and_encoding_split) &gt; 1:
            encoding_str = content_type_and_encoding_split[1].lower().strip()
            encoding = encoding_str.split(&#39;=&#39;)[-1]

        return content_type, encoding

    @staticmethod
    def _format_metadata_key(key: str) -&gt; str:
        &#34;&#34;&#34;
        Formats the fixed-key metadata keys as wanted by the multipart API.

        Ex: Content-Disposition --&gt; contentDisposition
        &#34;&#34;&#34;
        parts = key.split(&#39;-&#39;)
        parts = [parts[0].lower()] + [p.capitalize() for p in parts[1:]]
        return &#39;&#39;.join(parts)

    async def _download(self, bucket: str, object_name: str, *,
                        params: Optional[Dict[str, str]] = None,
                        headers: Optional[Dict[str, str]] = None,
                        timeout: int = DEFAULT_TIMEOUT,
                        session: Optional[Session] = None) -&gt; bytes:
        # https://cloud.google.com/storage/docs/request-endpoints#encoding
        encoded_object_name = quote(object_name, safe=&#39;&#39;)
        url = f&#39;{API_ROOT}/{bucket}/o/{encoded_object_name}&#39;
        headers = headers or {}
        headers.update(await self._headers())

        s = AioSession(session) if session else self.session
        response = await s.get(url, headers=headers, params=params or {},
                               timeout=timeout)

        # N.B. the GCS API sometimes returns &#39;application/octet-stream&#39; when a
        # string was uploaded. To avoid potential weirdness, always return a
        # bytes object.
        try:
            data: bytes = await response.read()
        except (AttributeError, TypeError):
            data = response.content  # type: ignore[assignment]

        return data

    async def _download_stream(self, bucket: str, object_name: str, *,
                               params: Optional[Dict[str, str]] = None,
                               headers: Optional[Dict[str, str]] = None,
                               timeout: int = DEFAULT_TIMEOUT,
                               session: Optional[Session] = None
                               ) -&gt; StreamResponse:
        # https://cloud.google.com/storage/docs/request-endpoints#encoding
        encoded_object_name = quote(object_name, safe=&#39;&#39;)
        url = f&#39;{API_ROOT}/{bucket}/o/{encoded_object_name}&#39;
        headers = headers or {}
        headers.update(await self._headers())

        s = AioSession(session) if session else self.session

        if BUILD_GCLOUD_REST:
            # stream argument is only expected by requests.Session.
            # pylint: disable=unexpected-keyword-arg
            return StreamResponse(s.get(url, headers=headers,
                                        params=params or {},
                                        timeout=timeout, stream=True))
        return StreamResponse(await s.get(url, headers=headers,
                                          params=params or {},
                                          timeout=timeout))

    async def _upload_simple(self, url: str, object_name: str,
                             stream: IO[AnyStr], params: Dict[str, str],
                             headers: Dict[str, str], *,
                             session: Optional[Session] = None,
                             timeout: int = 30) -&gt; Dict[str, Any]:
        # https://cloud.google.com/storage/docs/json_api/v1/how-tos/simple-upload
        params[&#39;name&#39;] = object_name
        params[&#39;uploadType&#39;] = &#39;media&#39;

        s = AioSession(session) if session else self.session
        # TODO: the type issue will be fixed in auth-4.0.2
        resp = await s.post(url, data=stream,  # type: ignore[arg-type]
                            headers=headers, params=params, timeout=timeout)
        data: Dict[str, Any] = await resp.json(content_type=None)
        return data

    async def _upload_multipart(self, url: str, object_name: str,
                                stream: IO[AnyStr], params: Dict[str, str],
                                headers: Dict[str, str],
                                metadata: Dict[str, Any], *,
                                session: Optional[Session] = None,
                                timeout: int = 30) -&gt; Dict[str, Any]:
        # https://cloud.google.com/storage/docs/json_api/v1/how-tos/multipart-upload
        params[&#39;uploadType&#39;] = &#39;multipart&#39;

        metadata_headers = {&#39;Content-Type&#39;: &#39;application/json; charset=UTF-8&#39;}
        metadata = {self._format_metadata_key(k): v
                    for k, v in metadata.items()}
        if &#39;metadata&#39; in metadata:
            metadata[&#39;metadata&#39;] = {
                str(k): str(v) if v is not None else None
                for k, v in metadata[&#39;metadata&#39;].items()}

        metadata[&#39;name&#39;] = object_name

        raw_body: AnyStr = stream.read()
        if isinstance(raw_body, str):
            bytes_body: bytes = raw_body.encode(&#39;utf-8&#39;)
        else:
            bytes_body = raw_body

        parts = [
            (metadata_headers, json.dumps(metadata).encode(&#39;utf-8&#39;)),
            ({&#39;Content-Type&#39;: headers[&#39;Content-Type&#39;]}, bytes_body),
        ]
        boundary = choose_boundary()
        body, content_type = encode_multipart_formdata(parts, boundary)
        headers.update({
            &#39;Content-Type&#39;: content_type,
            &#39;Content-Length&#39;: str(len(body)),
            &#39;Accept&#39;: &#39;application/json&#39;
        })

        s = AioSession(session) if session else self.session
        if not BUILD_GCLOUD_REST:
            # Wrap data in BytesIO to ensure aiohttp does not emit warning
            # when payload size &gt; 1MB
            body = io.BytesIO(body)  # type: ignore[assignment]

        # TODO: the type issue will be fixed in auth-4.0.2
        resp = await s.post(url, data=body,  # type: ignore[arg-type]
                            headers=headers, params=params, timeout=timeout)
        data: Dict[str, Any] = await resp.json(content_type=None)
        return data

    async def _upload_resumable(self, url: str, object_name: str,
                                stream: IO[AnyStr], params: Dict[str, str],
                                headers: Dict[str, str], *,
                                metadata: Optional[Dict[str, Any]] = None,
                                session: Optional[Session] = None,
                                timeout: int = 30) -&gt; Dict[str, Any]:
        # https://cloud.google.com/storage/docs/json_api/v1/how-tos/resumable-upload
        session_uri = await self._initiate_upload(url, object_name, params,
                                                  headers, metadata=metadata,
                                                  session=session)
        return await self._do_upload(session_uri, stream, headers=headers,
                                     session=session, timeout=timeout)

    async def _initiate_upload(self, url: str, object_name: str,
                               params: Dict[str, str], headers: Dict[str, str],
                               *, metadata: Optional[Dict[str, Any]] = None,
                               timeout: int = DEFAULT_TIMEOUT,
                               session: Optional[Session] = None) -&gt; str:
        params[&#39;uploadType&#39;] = &#39;resumable&#39;

        metadict = (metadata or {}).copy()
        metadict = {self._format_metadata_key(k): v
                    for k, v in metadict.items()}
        if &#39;metadata&#39; in metadict:
            metadict[&#39;metadata&#39;] = {
                str(k): str(v) if v is not None else None
                for k, v in metadict[&#39;metadata&#39;].items()}

        metadict.update({&#39;name&#39;: object_name})
        metadata_ = json.dumps(metadict)

        post_headers = headers.copy()
        post_headers.update({
            &#39;Content-Length&#39;: str(len(metadata_)),
            &#39;Content-Type&#39;: &#39;application/json; charset=UTF-8&#39;,
            &#39;X-Upload-Content-Type&#39;: headers[&#39;Content-Type&#39;],
            &#39;X-Upload-Content-Length&#39;: headers[&#39;Content-Length&#39;]
        })

        s = AioSession(session) if session else self.session
        resp = await s.post(url, headers=post_headers, params=params,
                            data=metadata_, timeout=timeout)
        session_uri: str = resp.headers[&#39;Location&#39;]
        return session_uri

    async def _do_upload(self, session_uri: str, stream: IO[AnyStr],
                         headers: Dict[str, str], *, retries: int = 5,
                         session: Optional[Session] = None,
                         timeout: int = 30) -&gt; Dict[str, Any]:
        s = AioSession(session) if session else self.session

        original_close = stream.close
        original_position = stream.tell()
        # Prevent the stream being closed if put operation fails
        stream.close = lambda: None  # type: ignore[assignment]
        try:
            for tries in range(retries):
                try:
                    resp = await s.put(session_uri, headers=headers,
                                       data=stream, timeout=timeout)
                except ResponseError:
                    headers.update({&#39;Content-Range&#39;: &#39;*/*&#39;})
                    stream.seek(original_position)

                    await sleep(  # type: ignore[func-returns-value]
                        2. ** tries
                    )
                else:
                    break
        finally:
            original_close()

        data: Dict[str, Any] = await resp.json(content_type=None)
        return data

    async def patch_metadata(
            self, bucket: str, object_name: str, metadata: Dict[str, Any],
            *, params: Optional[Dict[str, str]] = None,
            headers: Optional[Dict[str, str]] = None,
            session: Optional[Session] = None,
            timeout: int = DEFAULT_TIMEOUT) -&gt; Dict[str, Any]:
        # https://cloud.google.com/storage/docs/json_api/v1/objects/patch
        encoded_object_name = quote(object_name, safe=&#39;&#39;)
        url = f&#39;{API_ROOT}/{bucket}/o/{encoded_object_name}&#39;
        params = params or {}
        headers = headers or {}
        headers.update(await self._headers())
        headers[&#39;Content-Type&#39;] = &#39;application/json&#39;
        body = json.dumps(metadata).encode(&#39;utf-8&#39;)

        s = AioSession(session) if session else self.session
        # TODO: the type issue will be fixed in auth-4.0.2
        resp = await s.patch(url, data=body,  # type: ignore[arg-type]
                             headers=headers, params=params, timeout=timeout)
        data: Dict[str, Any] = await resp.json(content_type=None)
        return data

    async def get_bucket_metadata(self, bucket: str, *,
                                  params: Optional[Dict[str, str]] = None,
                                  headers: Optional[Dict[str, str]] = None,
                                  session: Optional[Session] = None,
                                  timeout: int = DEFAULT_TIMEOUT
                                  ) -&gt; Dict[str, Any]:
        url = f&#39;{API_ROOT}/{bucket}&#39;
        headers = headers or {}
        headers.update(await self._headers())

        s = AioSession(session) if session else self.session
        resp = await s.get(url, headers=headers, params=params or {},
                           timeout=timeout)
        data: Dict[str, Any] = await resp.json(content_type=None)
        return data

    async def close(self) -&gt; None:
        await self.session.close()

    async def __aenter__(self) -&gt; &#39;Storage&#39;:
        return self

    async def __aexit__(self, *args: Any) -&gt; None:
        await self.close()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="gcloud.aio.storage.storage.choose_boundary"><code class="name flex">
<span>def <span class="ident">choose_boundary</span></span>(<span>) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Stolen from urllib3.filepost.choose_boundary() as of v1.26.2.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def choose_boundary() -&gt; str:
    &#34;&#34;&#34;Stolen from urllib3.filepost.choose_boundary() as of v1.26.2.&#34;&#34;&#34;
    boundary = binascii.hexlify(os.urandom(16))
    if sys.version_info.major == 2:
        return boundary  # type: ignore[return-value]
    return boundary.decode(&#39;ascii&#39;)</code></pre>
</details>
</dd>
<dt id="gcloud.aio.storage.storage.encode_multipart_formdata"><code class="name flex">
<span>def <span class="ident">encode_multipart_formdata</span></span>(<span>fields: List[Tuple[Dict[str, str], bytes]], boundary: str) ‑> Tuple[bytes, str]</span>
</code></dt>
<dd>
<div class="desc"><p>Stolen from urllib3.filepost.encode_multipart_formdata() as of v1.26.2.</p>
<p>Very heavily modified to be compatible with our gcloud-rest converter and
to avoid unnecessary urllib3 dependencies (since that's only included with
requests, not aiohttp).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def encode_multipart_formdata(fields: List[Tuple[Dict[str, str], bytes]],
                              boundary: str) -&gt; Tuple[bytes, str]:
    &#34;&#34;&#34;
    Stolen from urllib3.filepost.encode_multipart_formdata() as of v1.26.2.

    Very heavily modified to be compatible with our gcloud-rest converter and
    to avoid unnecessary urllib3 dependencies (since that&#39;s only included with
    requests, not aiohttp).
    &#34;&#34;&#34;
    body: List[bytes] = []
    for headers, data in fields:
        body.append(f&#39;--{boundary}\r\n&#39;.encode(&#39;utf-8&#39;))

        # The below is from RequestFields.render_headers()
        # Since we only use Content-Type, we could simplify the below to a
        # single line... but probably best to be safe for future modifications.
        for field in [&#39;Content-Disposition&#39;, &#39;Content-Type&#39;,
                      &#39;Content-Location&#39;]:
            value = headers.pop(field, None)
            if value:
                body.append(f&#39;{field}: {value}\r\n&#39;.encode(&#39;utf-8&#39;))
        for field, value in headers.items():
            # N.B. potential bug copied from urllib3 code; zero values should
            # be sent! Keeping it for now, since Google libs use urllib3 for
            # their examples.
            if value:
                body.append(f&#39;{field}: {value}\r\n&#39;.encode(&#39;utf-8&#39;))

        body.append(b&#39;\r\n&#39;)
        body.append(data)
        body.append(b&#39;\r\n&#39;)

    body.append(f&#39;--{boundary}--\r\n&#39;.encode(&#39;utf-8&#39;))

    # N.B. &#39;multipart/form-data&#39; in upstream, but Google wants &#39;related&#39;
    content_type = f&#39;multipart/related; boundary={boundary}&#39;

    return b&#39;&#39;.join(body), content_type</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="gcloud.aio.storage.storage.Storage"><code class="flex name class">
<span>class <span class="ident">Storage</span></span>
<span>(</span><span>*, service_file: Union[str, IO[~AnyStr], ForwardRef(None)] = None, token: Optional[<a title="gcloud.aio.auth.token.Token" href="../auth/token.html#gcloud.aio.auth.token.Token">Token</a>] = None, session: Optional[aiohttp.client.ClientSession] = None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Storage:
    def __init__(self, *,
                 service_file: Optional[Union[str, IO[AnyStr]]] = None,
                 token: Optional[Token] = None,
                 session: Optional[Session] = None) -&gt; None:
        self.session = AioSession(session, verify_ssl=VERIFY_SSL)
        self.token = token or Token(
            service_file=service_file, scopes=SCOPES,
            session=self.session.session)  # type: ignore[arg-type]

    async def _headers(self) -&gt; Dict[str, str]:
        if STORAGE_EMULATOR_HOST:
            return {}

        token = await self.token.get()
        return {
            &#39;Authorization&#39;: f&#39;Bearer {token}&#39;,
        }

    def get_bucket(self, bucket_name: str) -&gt; Bucket:
        return Bucket(self, bucket_name)

    # pylint: disable=too-many-locals
    async def copy(self, bucket: str, object_name: str,
                   destination_bucket: str, *, new_name: Optional[str] = None,
                   metadata: Optional[Dict[str, Any]] = None,
                   params: Optional[Dict[str, str]] = None,
                   headers: Optional[Dict[str, str]] = None,
                   timeout: int = DEFAULT_TIMEOUT,
                   session: Optional[Session] = None) -&gt; Dict[str, Any]:

        &#34;&#34;&#34;
        When files are too large, multiple calls to `rewriteTo` are made. We
        refer to the same copy job by using the `rewriteToken` from the
        previous return payload in subsequent `rewriteTo` calls.

        Using the `rewriteTo` GCS API is preferred in part because it is able
        to make multiple calls to fully copy an object whereas the `copyTo` GCS
        API only calls `rewriteTo` once under the hood, and thus may fail if
        files are large.

        In the rare case you need to resume a copy operation, include the
        `rewriteToken` in the `params` dictionary. Once you begin a multi-part
        copy operation, you then have 1 week to complete the copy job.

        https://cloud.google.com/storage/docs/json_api/v1/objects/rewrite
        &#34;&#34;&#34;
        if not new_name:
            new_name = object_name

        url = (f&#34;{API_ROOT}/{bucket}/o/{quote(object_name, safe=&#39;&#39;)}/rewriteTo&#34;
               f&#34;/b/{destination_bucket}/o/{quote(new_name, safe=&#39;&#39;)}&#34;)

        # We may optionally supply metadata* to apply to the rewritten
        # object, which explains why `rewriteTo` is a POST endpoint; when no
        # metadata is given, we have to send an empty body.
        # * https://cloud.google.com/storage/docs/json_api/v1/objects#resource
        metadict = (metadata or {}).copy()
        metadict = {self._format_metadata_key(k): v
                    for k, v in metadict.items()}
        if &#39;metadata&#39; in metadict:
            metadict[&#39;metadata&#39;] = {
                str(k): str(v) if v is not None else None
                for k, v in metadict[&#39;metadata&#39;].items()}

        metadata_ = json.dumps(metadict)

        headers = headers or {}
        headers.update(await self._headers())
        headers.update({
            &#39;Content-Length&#39;: str(len(metadata_)),
            &#39;Content-Type&#39;: &#39;application/json; charset=UTF-8&#39;,
        })

        params = params or {}

        s = AioSession(session) if session else self.session
        resp = await s.post(url, headers=headers, params=params,
                            timeout=timeout, data=metadata_)

        data: Dict[str, Any] = await resp.json(content_type=None)

        while not data.get(&#39;done&#39;) and data.get(&#39;rewriteToken&#39;):
            params[&#39;rewriteToken&#39;] = data[&#39;rewriteToken&#39;]
            resp = await s.post(url, headers=headers, params=params,
                                timeout=timeout)
            data = await resp.json(content_type=None)

        return data

    async def delete(self, bucket: str, object_name: str, *,
                     timeout: int = DEFAULT_TIMEOUT,
                     params: Optional[Dict[str, str]] = None,
                     headers: Optional[Dict[str, str]] = None,
                     session: Optional[Session] = None) -&gt; str:
        # https://cloud.google.com/storage/docs/request-endpoints#encoding
        encoded_object_name = quote(object_name, safe=&#39;&#39;)
        url = f&#39;{API_ROOT}/{bucket}/o/{encoded_object_name}&#39;
        headers = headers or {}
        headers.update(await self._headers())

        s = AioSession(session) if session else self.session
        resp = await s.delete(url, headers=headers, params=params or {},
                              timeout=timeout)

        try:
            data: str = await resp.text()
        except (AttributeError, TypeError):
            data = str(resp.text)

        return data

    async def download(self, bucket: str, object_name: str, *,
                       headers: Optional[Dict[str, Any]] = None,
                       timeout: int = DEFAULT_TIMEOUT,
                       session: Optional[Session] = None) -&gt; bytes:
        return await self._download(bucket, object_name, headers=headers,
                                    timeout=timeout, params={&#39;alt&#39;: &#39;media&#39;},
                                    session=session)

    async def download_to_filename(self, bucket: str, object_name: str,
                                   filename: str, **kwargs: Any) -&gt; None:
        async with file_open(  # type: ignore[attr-defined]
                filename,
                mode=&#39;wb+&#39;,
        ) as file_object:
            await file_object.write(
                await self.download(bucket, object_name, **kwargs)
            )

    async def download_metadata(self, bucket: str, object_name: str, *,
                                headers: Optional[Dict[str, Any]] = None,
                                session: Optional[Session] = None,
                                timeout: int = DEFAULT_TIMEOUT
                                ) -&gt; Dict[str, Any]:
        data = await self._download(bucket, object_name, headers=headers,
                                    timeout=timeout, session=session)
        metadata: Dict[str, Any] = json.loads(data.decode())
        return metadata

    async def download_stream(self, bucket: str, object_name: str, *,
                              headers: Optional[Dict[str, Any]] = None,
                              timeout: int = DEFAULT_TIMEOUT,
                              session: Optional[Session] = None
                              ) -&gt; StreamResponse:
        &#34;&#34;&#34;Download a GCS object in a buffered stream.

        Args:
            bucket (str): The bucket from which to download.
            object_name (str): The object within the bucket to download.
            headers (Optional[Dict[str, Any]], optional): Custom header values
                for the request, such as range. Defaults to None.
            timeout (int, optional): Timeout, in seconds, for the request. Note
                that with this function, this is the time to the beginning of
                the response data (TTFB). Defaults to 10.
            session (Optional[Session], optional): A specific session to
                (re)use. Defaults to None.

        Returns:
            StreamResponse: A object encapsulating the stream, similar to
            io.BufferedIOBase, but it only supports the read() function.
        &#34;&#34;&#34;
        return await self._download_stream(bucket, object_name,
                                           headers=headers, timeout=timeout,
                                           params={&#39;alt&#39;: &#39;media&#39;},
                                           session=session)

    async def list_objects(self, bucket: str, *,
                           params: Optional[Dict[str, str]] = None,
                           headers: Optional[Dict[str, Any]] = None,
                           session: Optional[Session] = None,
                           timeout: int = DEFAULT_TIMEOUT) -&gt; Dict[str, Any]:
        url = f&#39;{API_ROOT}/{bucket}/o&#39;
        headers = headers or {}
        headers.update(await self._headers())

        s = AioSession(session) if session else self.session
        resp = await s.get(url, headers=headers, params=params or {},
                           timeout=timeout)
        data: Dict[str, Any] = await resp.json(content_type=None)
        return data

    # https://cloud.google.com/storage/docs/json_api/v1/how-tos/upload
    # pylint: disable=too-many-locals
    async def upload(self, bucket: str, object_name: str, file_data: Any,
                     *, content_type: Optional[str] = None,
                     parameters: Optional[Dict[str, str]] = None,
                     headers: Optional[Dict[str, str]] = None,
                     metadata: Optional[Dict[str, Any]] = None,
                     session: Optional[Session] = None,
                     force_resumable_upload: Optional[bool] = None,
                     timeout: int = 30) -&gt; Dict[str, Any]:
        url = f&#39;{API_ROOT_UPLOAD}/{bucket}/o&#39;

        stream = self._preprocess_data(file_data)

        if BUILD_GCLOUD_REST and isinstance(stream, io.StringIO):
            # HACK: `requests` library does not accept `str` as `data` in `put`
            # HTTP request.
            stream = io.BytesIO(stream.getvalue().encode(&#39;utf-8&#39;))

        content_length = self._get_stream_len(stream)

        # mime detection method same as in aiohttp 3.4.4
        content_type = content_type or mimetypes.guess_type(object_name)[0]

        parameters = parameters or {}

        headers = headers or {}
        headers.update(await self._headers())
        headers.update({
            &#39;Content-Length&#39;: str(content_length),
            &#39;Content-Type&#39;: content_type or &#39;&#39;,
        })

        upload_type = self._decide_upload_type(force_resumable_upload,
                                               content_length)
        log.debug(&#39;using %r gcloud storage upload method&#39;, upload_type)

        if upload_type == UploadType.RESUMABLE:
            return await self._upload_resumable(
                url, object_name, stream, parameters, headers,
                metadata=metadata, session=session, timeout=timeout)
        if upload_type == UploadType.SIMPLE:
            if metadata:
                return await self._upload_multipart(
                    url, object_name, stream, parameters, headers, metadata,
                    session=session, timeout=timeout)
            return await self._upload_simple(
                url, object_name, stream, parameters, headers, session=session,
                timeout=timeout)

        raise TypeError(f&#39;upload type {upload_type} not supported&#39;)

    async def upload_from_filename(self, bucket: str, object_name: str,
                                   filename: str,
                                   **kwargs: Any) -&gt; Dict[str, Any]:
        async with file_open(  # type: ignore[attr-defined]
                filename,
                mode=&#39;rb&#39;,
        ) as file_object:
            contents = await file_object.read()
            return await self.upload(bucket, object_name, contents,
                                     **kwargs)

    @staticmethod
    def _get_stream_len(stream: IO[AnyStr]) -&gt; int:
        current = stream.tell()
        try:
            return stream.seek(0, os.SEEK_END)
        finally:
            stream.seek(current)

    @staticmethod
    def _preprocess_data(data: Any) -&gt; IO[Any]:
        if data is None:
            return io.StringIO(&#39;&#39;)

        if isinstance(data, bytes):
            return io.BytesIO(data)
        if isinstance(data, str):
            return io.StringIO(data)
        if isinstance(data, io.IOBase):
            return data  # type: ignore[return-value]

        raise TypeError(f&#39;unsupported upload type: &#34;{type(data)}&#34;&#39;)

    @staticmethod
    def _decide_upload_type(force_resumable_upload: Optional[bool],
                            content_length: int) -&gt; UploadType:
        # force resumable
        if force_resumable_upload is True:
            return UploadType.RESUMABLE

        # force simple
        if force_resumable_upload is False:
            return UploadType.SIMPLE

        # decide based on Content-Length
        if content_length &gt; MAX_CONTENT_LENGTH_SIMPLE_UPLOAD:
            return UploadType.RESUMABLE

        return UploadType.SIMPLE

    @staticmethod
    def _split_content_type(content_type: str) -&gt; Tuple[str, Optional[str]]:
        content_type_and_encoding_split = content_type.split(&#39;;&#39;)
        content_type = content_type_and_encoding_split[0].lower().strip()

        encoding = None
        if len(content_type_and_encoding_split) &gt; 1:
            encoding_str = content_type_and_encoding_split[1].lower().strip()
            encoding = encoding_str.split(&#39;=&#39;)[-1]

        return content_type, encoding

    @staticmethod
    def _format_metadata_key(key: str) -&gt; str:
        &#34;&#34;&#34;
        Formats the fixed-key metadata keys as wanted by the multipart API.

        Ex: Content-Disposition --&gt; contentDisposition
        &#34;&#34;&#34;
        parts = key.split(&#39;-&#39;)
        parts = [parts[0].lower()] + [p.capitalize() for p in parts[1:]]
        return &#39;&#39;.join(parts)

    async def _download(self, bucket: str, object_name: str, *,
                        params: Optional[Dict[str, str]] = None,
                        headers: Optional[Dict[str, str]] = None,
                        timeout: int = DEFAULT_TIMEOUT,
                        session: Optional[Session] = None) -&gt; bytes:
        # https://cloud.google.com/storage/docs/request-endpoints#encoding
        encoded_object_name = quote(object_name, safe=&#39;&#39;)
        url = f&#39;{API_ROOT}/{bucket}/o/{encoded_object_name}&#39;
        headers = headers or {}
        headers.update(await self._headers())

        s = AioSession(session) if session else self.session
        response = await s.get(url, headers=headers, params=params or {},
                               timeout=timeout)

        # N.B. the GCS API sometimes returns &#39;application/octet-stream&#39; when a
        # string was uploaded. To avoid potential weirdness, always return a
        # bytes object.
        try:
            data: bytes = await response.read()
        except (AttributeError, TypeError):
            data = response.content  # type: ignore[assignment]

        return data

    async def _download_stream(self, bucket: str, object_name: str, *,
                               params: Optional[Dict[str, str]] = None,
                               headers: Optional[Dict[str, str]] = None,
                               timeout: int = DEFAULT_TIMEOUT,
                               session: Optional[Session] = None
                               ) -&gt; StreamResponse:
        # https://cloud.google.com/storage/docs/request-endpoints#encoding
        encoded_object_name = quote(object_name, safe=&#39;&#39;)
        url = f&#39;{API_ROOT}/{bucket}/o/{encoded_object_name}&#39;
        headers = headers or {}
        headers.update(await self._headers())

        s = AioSession(session) if session else self.session

        if BUILD_GCLOUD_REST:
            # stream argument is only expected by requests.Session.
            # pylint: disable=unexpected-keyword-arg
            return StreamResponse(s.get(url, headers=headers,
                                        params=params or {},
                                        timeout=timeout, stream=True))
        return StreamResponse(await s.get(url, headers=headers,
                                          params=params or {},
                                          timeout=timeout))

    async def _upload_simple(self, url: str, object_name: str,
                             stream: IO[AnyStr], params: Dict[str, str],
                             headers: Dict[str, str], *,
                             session: Optional[Session] = None,
                             timeout: int = 30) -&gt; Dict[str, Any]:
        # https://cloud.google.com/storage/docs/json_api/v1/how-tos/simple-upload
        params[&#39;name&#39;] = object_name
        params[&#39;uploadType&#39;] = &#39;media&#39;

        s = AioSession(session) if session else self.session
        # TODO: the type issue will be fixed in auth-4.0.2
        resp = await s.post(url, data=stream,  # type: ignore[arg-type]
                            headers=headers, params=params, timeout=timeout)
        data: Dict[str, Any] = await resp.json(content_type=None)
        return data

    async def _upload_multipart(self, url: str, object_name: str,
                                stream: IO[AnyStr], params: Dict[str, str],
                                headers: Dict[str, str],
                                metadata: Dict[str, Any], *,
                                session: Optional[Session] = None,
                                timeout: int = 30) -&gt; Dict[str, Any]:
        # https://cloud.google.com/storage/docs/json_api/v1/how-tos/multipart-upload
        params[&#39;uploadType&#39;] = &#39;multipart&#39;

        metadata_headers = {&#39;Content-Type&#39;: &#39;application/json; charset=UTF-8&#39;}
        metadata = {self._format_metadata_key(k): v
                    for k, v in metadata.items()}
        if &#39;metadata&#39; in metadata:
            metadata[&#39;metadata&#39;] = {
                str(k): str(v) if v is not None else None
                for k, v in metadata[&#39;metadata&#39;].items()}

        metadata[&#39;name&#39;] = object_name

        raw_body: AnyStr = stream.read()
        if isinstance(raw_body, str):
            bytes_body: bytes = raw_body.encode(&#39;utf-8&#39;)
        else:
            bytes_body = raw_body

        parts = [
            (metadata_headers, json.dumps(metadata).encode(&#39;utf-8&#39;)),
            ({&#39;Content-Type&#39;: headers[&#39;Content-Type&#39;]}, bytes_body),
        ]
        boundary = choose_boundary()
        body, content_type = encode_multipart_formdata(parts, boundary)
        headers.update({
            &#39;Content-Type&#39;: content_type,
            &#39;Content-Length&#39;: str(len(body)),
            &#39;Accept&#39;: &#39;application/json&#39;
        })

        s = AioSession(session) if session else self.session
        if not BUILD_GCLOUD_REST:
            # Wrap data in BytesIO to ensure aiohttp does not emit warning
            # when payload size &gt; 1MB
            body = io.BytesIO(body)  # type: ignore[assignment]

        # TODO: the type issue will be fixed in auth-4.0.2
        resp = await s.post(url, data=body,  # type: ignore[arg-type]
                            headers=headers, params=params, timeout=timeout)
        data: Dict[str, Any] = await resp.json(content_type=None)
        return data

    async def _upload_resumable(self, url: str, object_name: str,
                                stream: IO[AnyStr], params: Dict[str, str],
                                headers: Dict[str, str], *,
                                metadata: Optional[Dict[str, Any]] = None,
                                session: Optional[Session] = None,
                                timeout: int = 30) -&gt; Dict[str, Any]:
        # https://cloud.google.com/storage/docs/json_api/v1/how-tos/resumable-upload
        session_uri = await self._initiate_upload(url, object_name, params,
                                                  headers, metadata=metadata,
                                                  session=session)
        return await self._do_upload(session_uri, stream, headers=headers,
                                     session=session, timeout=timeout)

    async def _initiate_upload(self, url: str, object_name: str,
                               params: Dict[str, str], headers: Dict[str, str],
                               *, metadata: Optional[Dict[str, Any]] = None,
                               timeout: int = DEFAULT_TIMEOUT,
                               session: Optional[Session] = None) -&gt; str:
        params[&#39;uploadType&#39;] = &#39;resumable&#39;

        metadict = (metadata or {}).copy()
        metadict = {self._format_metadata_key(k): v
                    for k, v in metadict.items()}
        if &#39;metadata&#39; in metadict:
            metadict[&#39;metadata&#39;] = {
                str(k): str(v) if v is not None else None
                for k, v in metadict[&#39;metadata&#39;].items()}

        metadict.update({&#39;name&#39;: object_name})
        metadata_ = json.dumps(metadict)

        post_headers = headers.copy()
        post_headers.update({
            &#39;Content-Length&#39;: str(len(metadata_)),
            &#39;Content-Type&#39;: &#39;application/json; charset=UTF-8&#39;,
            &#39;X-Upload-Content-Type&#39;: headers[&#39;Content-Type&#39;],
            &#39;X-Upload-Content-Length&#39;: headers[&#39;Content-Length&#39;]
        })

        s = AioSession(session) if session else self.session
        resp = await s.post(url, headers=post_headers, params=params,
                            data=metadata_, timeout=timeout)
        session_uri: str = resp.headers[&#39;Location&#39;]
        return session_uri

    async def _do_upload(self, session_uri: str, stream: IO[AnyStr],
                         headers: Dict[str, str], *, retries: int = 5,
                         session: Optional[Session] = None,
                         timeout: int = 30) -&gt; Dict[str, Any]:
        s = AioSession(session) if session else self.session

        original_close = stream.close
        original_position = stream.tell()
        # Prevent the stream being closed if put operation fails
        stream.close = lambda: None  # type: ignore[assignment]
        try:
            for tries in range(retries):
                try:
                    resp = await s.put(session_uri, headers=headers,
                                       data=stream, timeout=timeout)
                except ResponseError:
                    headers.update({&#39;Content-Range&#39;: &#39;*/*&#39;})
                    stream.seek(original_position)

                    await sleep(  # type: ignore[func-returns-value]
                        2. ** tries
                    )
                else:
                    break
        finally:
            original_close()

        data: Dict[str, Any] = await resp.json(content_type=None)
        return data

    async def patch_metadata(
            self, bucket: str, object_name: str, metadata: Dict[str, Any],
            *, params: Optional[Dict[str, str]] = None,
            headers: Optional[Dict[str, str]] = None,
            session: Optional[Session] = None,
            timeout: int = DEFAULT_TIMEOUT) -&gt; Dict[str, Any]:
        # https://cloud.google.com/storage/docs/json_api/v1/objects/patch
        encoded_object_name = quote(object_name, safe=&#39;&#39;)
        url = f&#39;{API_ROOT}/{bucket}/o/{encoded_object_name}&#39;
        params = params or {}
        headers = headers or {}
        headers.update(await self._headers())
        headers[&#39;Content-Type&#39;] = &#39;application/json&#39;
        body = json.dumps(metadata).encode(&#39;utf-8&#39;)

        s = AioSession(session) if session else self.session
        # TODO: the type issue will be fixed in auth-4.0.2
        resp = await s.patch(url, data=body,  # type: ignore[arg-type]
                             headers=headers, params=params, timeout=timeout)
        data: Dict[str, Any] = await resp.json(content_type=None)
        return data

    async def get_bucket_metadata(self, bucket: str, *,
                                  params: Optional[Dict[str, str]] = None,
                                  headers: Optional[Dict[str, str]] = None,
                                  session: Optional[Session] = None,
                                  timeout: int = DEFAULT_TIMEOUT
                                  ) -&gt; Dict[str, Any]:
        url = f&#39;{API_ROOT}/{bucket}&#39;
        headers = headers or {}
        headers.update(await self._headers())

        s = AioSession(session) if session else self.session
        resp = await s.get(url, headers=headers, params=params or {},
                           timeout=timeout)
        data: Dict[str, Any] = await resp.json(content_type=None)
        return data

    async def close(self) -&gt; None:
        await self.session.close()

    async def __aenter__(self) -&gt; &#39;Storage&#39;:
        return self

    async def __aexit__(self, *args: Any) -&gt; None:
        await self.close()</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="gcloud.aio.storage.storage.Storage.close"><code class="name flex">
<span>async def <span class="ident">close</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def close(self) -&gt; None:
    await self.session.close()</code></pre>
</details>
</dd>
<dt id="gcloud.aio.storage.storage.Storage.copy"><code class="name flex">
<span>async def <span class="ident">copy</span></span>(<span>self, bucket: str, object_name: str, destination_bucket: str, *, new_name: Optional[str] = None, metadata: Optional[Dict[str, Any]] = None, params: Optional[Dict[str, str]] = None, headers: Optional[Dict[str, str]] = None, timeout: int = 10, session: Optional[aiohttp.client.ClientSession] = None) ‑> Dict[str, Any]</span>
</code></dt>
<dd>
<div class="desc"><p>When files are too large, multiple calls to <code>rewriteTo</code> are made. We
refer to the same copy job by using the <code>rewriteToken</code> from the
previous return payload in subsequent <code>rewriteTo</code> calls.</p>
<p>Using the <code>rewriteTo</code> GCS API is preferred in part because it is able
to make multiple calls to fully copy an object whereas the <code>copyTo</code> GCS
API only calls <code>rewriteTo</code> once under the hood, and thus may fail if
files are large.</p>
<p>In the rare case you need to resume a copy operation, include the
<code>rewriteToken</code> in the <code>params</code> dictionary. Once you begin a multi-part
copy operation, you then have 1 week to complete the copy job.</p>
<p><a href="https://cloud.google.com/storage/docs/json_api/v1/objects/rewrite">https://cloud.google.com/storage/docs/json_api/v1/objects/rewrite</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def copy(self, bucket: str, object_name: str,
               destination_bucket: str, *, new_name: Optional[str] = None,
               metadata: Optional[Dict[str, Any]] = None,
               params: Optional[Dict[str, str]] = None,
               headers: Optional[Dict[str, str]] = None,
               timeout: int = DEFAULT_TIMEOUT,
               session: Optional[Session] = None) -&gt; Dict[str, Any]:

    &#34;&#34;&#34;
    When files are too large, multiple calls to `rewriteTo` are made. We
    refer to the same copy job by using the `rewriteToken` from the
    previous return payload in subsequent `rewriteTo` calls.

    Using the `rewriteTo` GCS API is preferred in part because it is able
    to make multiple calls to fully copy an object whereas the `copyTo` GCS
    API only calls `rewriteTo` once under the hood, and thus may fail if
    files are large.

    In the rare case you need to resume a copy operation, include the
    `rewriteToken` in the `params` dictionary. Once you begin a multi-part
    copy operation, you then have 1 week to complete the copy job.

    https://cloud.google.com/storage/docs/json_api/v1/objects/rewrite
    &#34;&#34;&#34;
    if not new_name:
        new_name = object_name

    url = (f&#34;{API_ROOT}/{bucket}/o/{quote(object_name, safe=&#39;&#39;)}/rewriteTo&#34;
           f&#34;/b/{destination_bucket}/o/{quote(new_name, safe=&#39;&#39;)}&#34;)

    # We may optionally supply metadata* to apply to the rewritten
    # object, which explains why `rewriteTo` is a POST endpoint; when no
    # metadata is given, we have to send an empty body.
    # * https://cloud.google.com/storage/docs/json_api/v1/objects#resource
    metadict = (metadata or {}).copy()
    metadict = {self._format_metadata_key(k): v
                for k, v in metadict.items()}
    if &#39;metadata&#39; in metadict:
        metadict[&#39;metadata&#39;] = {
            str(k): str(v) if v is not None else None
            for k, v in metadict[&#39;metadata&#39;].items()}

    metadata_ = json.dumps(metadict)

    headers = headers or {}
    headers.update(await self._headers())
    headers.update({
        &#39;Content-Length&#39;: str(len(metadata_)),
        &#39;Content-Type&#39;: &#39;application/json; charset=UTF-8&#39;,
    })

    params = params or {}

    s = AioSession(session) if session else self.session
    resp = await s.post(url, headers=headers, params=params,
                        timeout=timeout, data=metadata_)

    data: Dict[str, Any] = await resp.json(content_type=None)

    while not data.get(&#39;done&#39;) and data.get(&#39;rewriteToken&#39;):
        params[&#39;rewriteToken&#39;] = data[&#39;rewriteToken&#39;]
        resp = await s.post(url, headers=headers, params=params,
                            timeout=timeout)
        data = await resp.json(content_type=None)

    return data</code></pre>
</details>
</dd>
<dt id="gcloud.aio.storage.storage.Storage.delete"><code class="name flex">
<span>async def <span class="ident">delete</span></span>(<span>self, bucket: str, object_name: str, *, timeout: int = 10, params: Optional[Dict[str, str]] = None, headers: Optional[Dict[str, str]] = None, session: Optional[aiohttp.client.ClientSession] = None) ‑> str</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def delete(self, bucket: str, object_name: str, *,
                 timeout: int = DEFAULT_TIMEOUT,
                 params: Optional[Dict[str, str]] = None,
                 headers: Optional[Dict[str, str]] = None,
                 session: Optional[Session] = None) -&gt; str:
    # https://cloud.google.com/storage/docs/request-endpoints#encoding
    encoded_object_name = quote(object_name, safe=&#39;&#39;)
    url = f&#39;{API_ROOT}/{bucket}/o/{encoded_object_name}&#39;
    headers = headers or {}
    headers.update(await self._headers())

    s = AioSession(session) if session else self.session
    resp = await s.delete(url, headers=headers, params=params or {},
                          timeout=timeout)

    try:
        data: str = await resp.text()
    except (AttributeError, TypeError):
        data = str(resp.text)

    return data</code></pre>
</details>
</dd>
<dt id="gcloud.aio.storage.storage.Storage.download"><code class="name flex">
<span>async def <span class="ident">download</span></span>(<span>self, bucket: str, object_name: str, *, headers: Optional[Dict[str, Any]] = None, timeout: int = 10, session: Optional[aiohttp.client.ClientSession] = None) ‑> bytes</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def download(self, bucket: str, object_name: str, *,
                   headers: Optional[Dict[str, Any]] = None,
                   timeout: int = DEFAULT_TIMEOUT,
                   session: Optional[Session] = None) -&gt; bytes:
    return await self._download(bucket, object_name, headers=headers,
                                timeout=timeout, params={&#39;alt&#39;: &#39;media&#39;},
                                session=session)</code></pre>
</details>
</dd>
<dt id="gcloud.aio.storage.storage.Storage.download_metadata"><code class="name flex">
<span>async def <span class="ident">download_metadata</span></span>(<span>self, bucket: str, object_name: str, *, headers: Optional[Dict[str, Any]] = None, session: Optional[aiohttp.client.ClientSession] = None, timeout: int = 10) ‑> Dict[str, Any]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def download_metadata(self, bucket: str, object_name: str, *,
                            headers: Optional[Dict[str, Any]] = None,
                            session: Optional[Session] = None,
                            timeout: int = DEFAULT_TIMEOUT
                            ) -&gt; Dict[str, Any]:
    data = await self._download(bucket, object_name, headers=headers,
                                timeout=timeout, session=session)
    metadata: Dict[str, Any] = json.loads(data.decode())
    return metadata</code></pre>
</details>
</dd>
<dt id="gcloud.aio.storage.storage.Storage.download_stream"><code class="name flex">
<span>async def <span class="ident">download_stream</span></span>(<span>self, bucket: str, object_name: str, *, headers: Optional[Dict[str, Any]] = None, timeout: int = 10, session: Optional[aiohttp.client.ClientSession] = None) ‑> <a title="gcloud.aio.storage.storage.StreamResponse" href="#gcloud.aio.storage.storage.StreamResponse">StreamResponse</a></span>
</code></dt>
<dd>
<div class="desc"><p>Download a GCS object in a buffered stream.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>bucket</code></strong> :&ensp;<code>str</code></dt>
<dd>The bucket from which to download.</dd>
<dt><strong><code>object_name</code></strong> :&ensp;<code>str</code></dt>
<dd>The object within the bucket to download.</dd>
<dt><strong><code>headers</code></strong> :&ensp;<code>Optional[Dict[str, Any]]</code>, optional</dt>
<dd>Custom header values
for the request, such as range. Defaults to None.</dd>
<dt><strong><code>timeout</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Timeout, in seconds, for the request. Note
that with this function, this is the time to the beginning of
the response data (TTFB). Defaults to 10.</dd>
<dt><strong><code>session</code></strong> :&ensp;<code>Optional[Session]</code>, optional</dt>
<dd>A specific session to
(re)use. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="gcloud.aio.storage.storage.StreamResponse" href="#gcloud.aio.storage.storage.StreamResponse">StreamResponse</a></code></dt>
<dd>A object encapsulating the stream, similar to</dd>
</dl>
<p>io.BufferedIOBase, but it only supports the read() function.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def download_stream(self, bucket: str, object_name: str, *,
                          headers: Optional[Dict[str, Any]] = None,
                          timeout: int = DEFAULT_TIMEOUT,
                          session: Optional[Session] = None
                          ) -&gt; StreamResponse:
    &#34;&#34;&#34;Download a GCS object in a buffered stream.

    Args:
        bucket (str): The bucket from which to download.
        object_name (str): The object within the bucket to download.
        headers (Optional[Dict[str, Any]], optional): Custom header values
            for the request, such as range. Defaults to None.
        timeout (int, optional): Timeout, in seconds, for the request. Note
            that with this function, this is the time to the beginning of
            the response data (TTFB). Defaults to 10.
        session (Optional[Session], optional): A specific session to
            (re)use. Defaults to None.

    Returns:
        StreamResponse: A object encapsulating the stream, similar to
        io.BufferedIOBase, but it only supports the read() function.
    &#34;&#34;&#34;
    return await self._download_stream(bucket, object_name,
                                       headers=headers, timeout=timeout,
                                       params={&#39;alt&#39;: &#39;media&#39;},
                                       session=session)</code></pre>
</details>
</dd>
<dt id="gcloud.aio.storage.storage.Storage.download_to_filename"><code class="name flex">
<span>async def <span class="ident">download_to_filename</span></span>(<span>self, bucket: str, object_name: str, filename: str, **kwargs: Any) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def download_to_filename(self, bucket: str, object_name: str,
                               filename: str, **kwargs: Any) -&gt; None:
    async with file_open(  # type: ignore[attr-defined]
            filename,
            mode=&#39;wb+&#39;,
    ) as file_object:
        await file_object.write(
            await self.download(bucket, object_name, **kwargs)
        )</code></pre>
</details>
</dd>
<dt id="gcloud.aio.storage.storage.Storage.get_bucket"><code class="name flex">
<span>def <span class="ident">get_bucket</span></span>(<span>self, bucket_name: str) ‑> <a title="gcloud.aio.storage.bucket.Bucket" href="bucket.html#gcloud.aio.storage.bucket.Bucket">Bucket</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_bucket(self, bucket_name: str) -&gt; Bucket:
    return Bucket(self, bucket_name)</code></pre>
</details>
</dd>
<dt id="gcloud.aio.storage.storage.Storage.get_bucket_metadata"><code class="name flex">
<span>async def <span class="ident">get_bucket_metadata</span></span>(<span>self, bucket: str, *, params: Optional[Dict[str, str]] = None, headers: Optional[Dict[str, str]] = None, session: Optional[aiohttp.client.ClientSession] = None, timeout: int = 10) ‑> Dict[str, Any]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def get_bucket_metadata(self, bucket: str, *,
                              params: Optional[Dict[str, str]] = None,
                              headers: Optional[Dict[str, str]] = None,
                              session: Optional[Session] = None,
                              timeout: int = DEFAULT_TIMEOUT
                              ) -&gt; Dict[str, Any]:
    url = f&#39;{API_ROOT}/{bucket}&#39;
    headers = headers or {}
    headers.update(await self._headers())

    s = AioSession(session) if session else self.session
    resp = await s.get(url, headers=headers, params=params or {},
                       timeout=timeout)
    data: Dict[str, Any] = await resp.json(content_type=None)
    return data</code></pre>
</details>
</dd>
<dt id="gcloud.aio.storage.storage.Storage.list_objects"><code class="name flex">
<span>async def <span class="ident">list_objects</span></span>(<span>self, bucket: str, *, params: Optional[Dict[str, str]] = None, headers: Optional[Dict[str, Any]] = None, session: Optional[aiohttp.client.ClientSession] = None, timeout: int = 10) ‑> Dict[str, Any]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def list_objects(self, bucket: str, *,
                       params: Optional[Dict[str, str]] = None,
                       headers: Optional[Dict[str, Any]] = None,
                       session: Optional[Session] = None,
                       timeout: int = DEFAULT_TIMEOUT) -&gt; Dict[str, Any]:
    url = f&#39;{API_ROOT}/{bucket}/o&#39;
    headers = headers or {}
    headers.update(await self._headers())

    s = AioSession(session) if session else self.session
    resp = await s.get(url, headers=headers, params=params or {},
                       timeout=timeout)
    data: Dict[str, Any] = await resp.json(content_type=None)
    return data</code></pre>
</details>
</dd>
<dt id="gcloud.aio.storage.storage.Storage.patch_metadata"><code class="name flex">
<span>async def <span class="ident">patch_metadata</span></span>(<span>self, bucket: str, object_name: str, metadata: Dict[str, Any], *, params: Optional[Dict[str, str]] = None, headers: Optional[Dict[str, str]] = None, session: Optional[aiohttp.client.ClientSession] = None, timeout: int = 10) ‑> Dict[str, Any]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def patch_metadata(
        self, bucket: str, object_name: str, metadata: Dict[str, Any],
        *, params: Optional[Dict[str, str]] = None,
        headers: Optional[Dict[str, str]] = None,
        session: Optional[Session] = None,
        timeout: int = DEFAULT_TIMEOUT) -&gt; Dict[str, Any]:
    # https://cloud.google.com/storage/docs/json_api/v1/objects/patch
    encoded_object_name = quote(object_name, safe=&#39;&#39;)
    url = f&#39;{API_ROOT}/{bucket}/o/{encoded_object_name}&#39;
    params = params or {}
    headers = headers or {}
    headers.update(await self._headers())
    headers[&#39;Content-Type&#39;] = &#39;application/json&#39;
    body = json.dumps(metadata).encode(&#39;utf-8&#39;)

    s = AioSession(session) if session else self.session
    # TODO: the type issue will be fixed in auth-4.0.2
    resp = await s.patch(url, data=body,  # type: ignore[arg-type]
                         headers=headers, params=params, timeout=timeout)
    data: Dict[str, Any] = await resp.json(content_type=None)
    return data</code></pre>
</details>
</dd>
<dt id="gcloud.aio.storage.storage.Storage.upload"><code class="name flex">
<span>async def <span class="ident">upload</span></span>(<span>self, bucket: str, object_name: str, file_data: Any, *, content_type: Optional[str] = None, parameters: Optional[Dict[str, str]] = None, headers: Optional[Dict[str, str]] = None, metadata: Optional[Dict[str, Any]] = None, session: Optional[aiohttp.client.ClientSession] = None, force_resumable_upload: Optional[bool] = None, timeout: int = 30) ‑> Dict[str, Any]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def upload(self, bucket: str, object_name: str, file_data: Any,
                 *, content_type: Optional[str] = None,
                 parameters: Optional[Dict[str, str]] = None,
                 headers: Optional[Dict[str, str]] = None,
                 metadata: Optional[Dict[str, Any]] = None,
                 session: Optional[Session] = None,
                 force_resumable_upload: Optional[bool] = None,
                 timeout: int = 30) -&gt; Dict[str, Any]:
    url = f&#39;{API_ROOT_UPLOAD}/{bucket}/o&#39;

    stream = self._preprocess_data(file_data)

    if BUILD_GCLOUD_REST and isinstance(stream, io.StringIO):
        # HACK: `requests` library does not accept `str` as `data` in `put`
        # HTTP request.
        stream = io.BytesIO(stream.getvalue().encode(&#39;utf-8&#39;))

    content_length = self._get_stream_len(stream)

    # mime detection method same as in aiohttp 3.4.4
    content_type = content_type or mimetypes.guess_type(object_name)[0]

    parameters = parameters or {}

    headers = headers or {}
    headers.update(await self._headers())
    headers.update({
        &#39;Content-Length&#39;: str(content_length),
        &#39;Content-Type&#39;: content_type or &#39;&#39;,
    })

    upload_type = self._decide_upload_type(force_resumable_upload,
                                           content_length)
    log.debug(&#39;using %r gcloud storage upload method&#39;, upload_type)

    if upload_type == UploadType.RESUMABLE:
        return await self._upload_resumable(
            url, object_name, stream, parameters, headers,
            metadata=metadata, session=session, timeout=timeout)
    if upload_type == UploadType.SIMPLE:
        if metadata:
            return await self._upload_multipart(
                url, object_name, stream, parameters, headers, metadata,
                session=session, timeout=timeout)
        return await self._upload_simple(
            url, object_name, stream, parameters, headers, session=session,
            timeout=timeout)

    raise TypeError(f&#39;upload type {upload_type} not supported&#39;)</code></pre>
</details>
</dd>
<dt id="gcloud.aio.storage.storage.Storage.upload_from_filename"><code class="name flex">
<span>async def <span class="ident">upload_from_filename</span></span>(<span>self, bucket: str, object_name: str, filename: str, **kwargs: Any) ‑> Dict[str, Any]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def upload_from_filename(self, bucket: str, object_name: str,
                               filename: str,
                               **kwargs: Any) -&gt; Dict[str, Any]:
    async with file_open(  # type: ignore[attr-defined]
            filename,
            mode=&#39;rb&#39;,
    ) as file_object:
        contents = await file_object.read()
        return await self.upload(bucket, object_name, contents,
                                 **kwargs)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="gcloud.aio.storage.storage.StreamResponse"><code class="flex name class">
<span>class <span class="ident">StreamResponse</span></span>
<span>(</span><span>response: Any)</span>
</code></dt>
<dd>
<div class="desc"><p>This class provides an abstraction between the slightly different
recommended streaming implementations between requests and aiohttp.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class StreamResponse:
    &#34;&#34;&#34;This class provides an abstraction between the slightly different
    recommended streaming implementations between requests and aiohttp.
    &#34;&#34;&#34;

    def __init__(self, response: Any) -&gt; None:
        self._response = response
        self._iter: Optional[Iterator[bytes]] = None

    @property
    def content_length(self) -&gt; int:
        return int(self._response.headers.get(&#39;content-length&#39;, 0))

    async def read(self, size: int = -1) -&gt; bytes:
        chunk: bytes
        if BUILD_GCLOUD_REST:
            if self._iter is None:
                self._iter = self._response.iter_content(chunk_size=size)
            chunk = next(self._iter, b&#39;&#39;)
        else:
            chunk = await self._response.content.read(size)
        return chunk

    async def __aenter__(self) -&gt; Any:
        # strictly speaking, since this method can&#39;t be called via gcloud-rest,
        # we know the return type is aiohttp.ClientResponse
        return await self._response.__aenter__()

    async def __aexit__(self, *exc_info: Any) -&gt; None:
        await self._response.__aexit__(*exc_info)</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="gcloud.aio.storage.storage.StreamResponse.content_length"><code class="name">var <span class="ident">content_length</span> : int</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def content_length(self) -&gt; int:
    return int(self._response.headers.get(&#39;content-length&#39;, 0))</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="gcloud.aio.storage.storage.StreamResponse.read"><code class="name flex">
<span>async def <span class="ident">read</span></span>(<span>self, size: int = -1) ‑> bytes</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def read(self, size: int = -1) -&gt; bytes:
    chunk: bytes
    if BUILD_GCLOUD_REST:
        if self._iter is None:
            self._iter = self._response.iter_content(chunk_size=size)
        chunk = next(self._iter, b&#39;&#39;)
    else:
        chunk = await self._response.content.read(size)
    return chunk</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="gcloud.aio.storage.storage.UploadType"><code class="flex name class">
<span>class <span class="ident">UploadType</span></span>
<span>(</span><span>value, names=None, *, module=None, qualname=None, type=None, start=1)</span>
</code></dt>
<dd>
<div class="desc"><p>An enumeration.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class UploadType(enum.Enum):
    SIMPLE = 1
    RESUMABLE = 2
    MULTIPART = 3  # unused: SIMPLE upgrades to MULTIPART when metadata exists</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="gcloud.aio.storage.storage.UploadType.MULTIPART"><code class="name">var <span class="ident">MULTIPART</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="gcloud.aio.storage.storage.UploadType.RESUMABLE"><code class="name">var <span class="ident">RESUMABLE</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="gcloud.aio.storage.storage.UploadType.SIMPLE"><code class="name">var <span class="ident">SIMPLE</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="gcloud.aio.storage" href="index.html">gcloud.aio.storage</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="gcloud.aio.storage.storage.choose_boundary" href="#gcloud.aio.storage.storage.choose_boundary">choose_boundary</a></code></li>
<li><code><a title="gcloud.aio.storage.storage.encode_multipart_formdata" href="#gcloud.aio.storage.storage.encode_multipart_formdata">encode_multipart_formdata</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="gcloud.aio.storage.storage.Storage" href="#gcloud.aio.storage.storage.Storage">Storage</a></code></h4>
<ul class="">
<li><code><a title="gcloud.aio.storage.storage.Storage.close" href="#gcloud.aio.storage.storage.Storage.close">close</a></code></li>
<li><code><a title="gcloud.aio.storage.storage.Storage.copy" href="#gcloud.aio.storage.storage.Storage.copy">copy</a></code></li>
<li><code><a title="gcloud.aio.storage.storage.Storage.delete" href="#gcloud.aio.storage.storage.Storage.delete">delete</a></code></li>
<li><code><a title="gcloud.aio.storage.storage.Storage.download" href="#gcloud.aio.storage.storage.Storage.download">download</a></code></li>
<li><code><a title="gcloud.aio.storage.storage.Storage.download_metadata" href="#gcloud.aio.storage.storage.Storage.download_metadata">download_metadata</a></code></li>
<li><code><a title="gcloud.aio.storage.storage.Storage.download_stream" href="#gcloud.aio.storage.storage.Storage.download_stream">download_stream</a></code></li>
<li><code><a title="gcloud.aio.storage.storage.Storage.download_to_filename" href="#gcloud.aio.storage.storage.Storage.download_to_filename">download_to_filename</a></code></li>
<li><code><a title="gcloud.aio.storage.storage.Storage.get_bucket" href="#gcloud.aio.storage.storage.Storage.get_bucket">get_bucket</a></code></li>
<li><code><a title="gcloud.aio.storage.storage.Storage.get_bucket_metadata" href="#gcloud.aio.storage.storage.Storage.get_bucket_metadata">get_bucket_metadata</a></code></li>
<li><code><a title="gcloud.aio.storage.storage.Storage.list_objects" href="#gcloud.aio.storage.storage.Storage.list_objects">list_objects</a></code></li>
<li><code><a title="gcloud.aio.storage.storage.Storage.patch_metadata" href="#gcloud.aio.storage.storage.Storage.patch_metadata">patch_metadata</a></code></li>
<li><code><a title="gcloud.aio.storage.storage.Storage.upload" href="#gcloud.aio.storage.storage.Storage.upload">upload</a></code></li>
<li><code><a title="gcloud.aio.storage.storage.Storage.upload_from_filename" href="#gcloud.aio.storage.storage.Storage.upload_from_filename">upload_from_filename</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="gcloud.aio.storage.storage.StreamResponse" href="#gcloud.aio.storage.storage.StreamResponse">StreamResponse</a></code></h4>
<ul class="">
<li><code><a title="gcloud.aio.storage.storage.StreamResponse.content_length" href="#gcloud.aio.storage.storage.StreamResponse.content_length">content_length</a></code></li>
<li><code><a title="gcloud.aio.storage.storage.StreamResponse.read" href="#gcloud.aio.storage.storage.StreamResponse.read">read</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="gcloud.aio.storage.storage.UploadType" href="#gcloud.aio.storage.storage.UploadType">UploadType</a></code></h4>
<ul class="">
<li><code><a title="gcloud.aio.storage.storage.UploadType.MULTIPART" href="#gcloud.aio.storage.storage.UploadType.MULTIPART">MULTIPART</a></code></li>
<li><code><a title="gcloud.aio.storage.storage.UploadType.RESUMABLE" href="#gcloud.aio.storage.storage.UploadType.RESUMABLE">RESUMABLE</a></code></li>
<li><code><a title="gcloud.aio.storage.storage.UploadType.SIMPLE" href="#gcloud.aio.storage.storage.UploadType.SIMPLE">SIMPLE</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>